{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**I - Simple EQL**\n",
        "  including the following symbolic operations {multiplication, sin, sign, ln}"
      ],
      "metadata": {
        "id": "2KwPBo7LcVbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **I.1 - Base Functions**"
      ],
      "metadata": {
        "id": "l2zgxyH8panE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Functions for use with Symbolic Regression \"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import sympy as sym\n",
        "\n",
        "class BaseFunction :\n",
        "  \"\"\"Abstract parent class for primitive symbolic functions\"\"\"\n",
        "\n",
        "  def __init__(self,norm=1) :\n",
        "    self.norm=norm\n",
        "\n",
        "\n",
        "  def sp(self,x) :\n",
        "    return None\n",
        "\n",
        "  def torch(self,x) :\n",
        "    return None\n",
        "\n",
        "  def name(self,x) :\n",
        "    \"\"\"Sympy to String\"\"\"\n",
        "    return str(self.sp)\n",
        "\n",
        "  def np(self,x) :\n",
        "    \"\"\"Sympy to Numpy\"\"\"\n",
        "    z=sp.symbols('z')\n",
        "    return sym.utilities.lambdify(z,self.sp(z),'numpy')(x)\n",
        "\n",
        "\n",
        "class Sin(BaseFunction) :\n",
        "  def torch(self, x) :\n",
        "    return torch.sin(4*np.pi*x) /self.norm\n",
        "\n",
        "  def sp(self, x) :\n",
        "    return sym.sin(4*np.pi*x) /self.norm\n",
        "\n",
        "class Log(BaseFunction) : #do not forget that we work in R+\n",
        "  def torch(self, x) :\n",
        "    return torch.log(torch.abs(x)) /self.norm\n",
        "\n",
        "  def sp(self, x) :\n",
        "    return sym.log(sym.Abs(x)) /self.norm\n",
        "\n",
        "class Sign(BaseFunction) :\n",
        "  def torch(self, x) :\n",
        "    return torch.sign(x) /self.norm\n",
        "\n",
        "  def sp(self, x) :\n",
        "    return sym.sign(x) /self.norm\n",
        "\n",
        "\n",
        "class Identity(BaseFunction) :\n",
        "  def torch(self, x) :\n",
        "    return x /self.norm\n",
        "\n",
        "  def sp(self, x) :\n",
        "    return x /self.norm\n",
        "\n",
        "class Constant(BaseFunction) :\n",
        "  def torch(self, x) :\n",
        "    return torch.ones_like(x)\n",
        "\n",
        "  def sp(self, x) :\n",
        "    return 1\n",
        "\n",
        "  def np(self,x) :\n",
        "    return np.ones_like\n",
        "\n",
        "\n",
        "class Square(BaseFunction) :\n",
        "  def torch(self, x) :\n",
        "    return torch.square(x) /self.norm\n",
        "\n",
        "  def sp(self, x) :\n",
        "    return x**2 /self.norm\n",
        "\n",
        "\n",
        "class BaseFunction2 :\n",
        "  \"\"\"Abstract parent class for primitive symbolic functions WITH 2 VARIABLES\"\"\"\n",
        "\n",
        "  def __init__(self,norm=1) :\n",
        "    self.norm=norm\n",
        "\n",
        "  def sp(self,x,y) :\n",
        "    return None\n",
        "\n",
        "  def torch(self,x,y) :\n",
        "    return None\n",
        "\n",
        "  def name(self,x,y) :\n",
        "    \"\"\"Sympy to String\"\"\"\n",
        "    return str(self.sp)\n",
        "\n",
        "  def np(self,x,y) :\n",
        "    \"\"\"Sympy to Numpy\"\"\"\n",
        "    a,b=sp.symbols('z w')\n",
        "    return sym.utilities.lambdify([a,b],self.sp(a,b),'numpy')(x,y)\n",
        "\n",
        "class Product(BaseFunction2) :\n",
        "  def __init__(self,norm=0.1) :\n",
        "    super().__init__(norm=norm) # do not forget how to use super\n",
        "\n",
        "  def torch(self,x,y) :\n",
        "    return x*y / self.norm\n",
        "\n",
        "  def sp(self,x,y) :\n",
        "    return x*y / self.norm\n",
        "\n",
        "\n",
        "def count_inputs(funcs) :\n",
        "  i=0\n",
        "  for func in funcs :\n",
        "    if isinstance(func,BaseFunction) :\n",
        "      i+=1\n",
        "    elif isinstance(func,BaseFunction2) :\n",
        "      i+=2\n",
        "  return i\n",
        "\n",
        "def count_double(funcs) :\n",
        "  i=0\n",
        "  for func in funcs :\n",
        "    if isinstance(func,BaseFunction2) :\n",
        "      i+=1\n",
        "  return i\n",
        "\n",
        "\n",
        "# default_func= [\n",
        "#    Sin()\n",
        "#    ...\n",
        "#    ]\n",
        "\n",
        "default_func = [      # The number of times each symbolic primitive appear fix its probability of being chosen by the network\n",
        "    *[Constant()] * 2,\n",
        "    *[Identity()] * 4,\n",
        "    *[Square()] * 4,\n",
        "    *[Sin()] * 2,\n",
        "    *[Sign()] * 2,\n",
        "    *[Log()] * 2,\n",
        "    *[Product()] * 2, #2 inputs = 1 output at the end of default_func\n",
        "]"
      ],
      "metadata": {
        "id": "Us7eAsBepczT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **I.2 - EQL Network**\n"
      ],
      "metadata": {
        "id": "H7RLNJvag7Kd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Without Regularization*"
      ],
      "metadata": {
        "id": "a_X9uhgkCpwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Contains the symbolic regression neural network architecture.\"\"\"\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#from utils import functions as functions\n",
        "\n",
        "\n",
        "class SymbolicLayer(nn.Module):\n",
        "    \"\"\"Neural network layer for symbolic regression where activation functions correspond to primitive functions.\n",
        "    Can take multi-input activation functions (like multiplication)\"\"\"\n",
        "    def __init__(self, funcs=None, initial_weight=None, init_stddev=0.1, in_dim=None):\n",
        "        \"\"\"\n",
        "        funcs: List of activation functions, using utils.functions\n",
        "        initial_weight: (Optional) Initial value for weight matrix\n",
        "        variable: Boolean of whether initial_weight is a variable or not\n",
        "        init_stddev: (Optional) if initial_weight isn't passed in, this is standard deviation of initial weight\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        if funcs is None:\n",
        "            funcs = default_func\n",
        "        self.initial_weight = initial_weight\n",
        "        self.W = None       # Weight matrix\n",
        "        self.built = False  # Boolean whether weights have been initialized\n",
        "\n",
        "        self.output = None  # tensor for layer output\n",
        "        self.n_funcs = len(funcs)                       # Number of activation functions (and number of layer outputs)\n",
        "        self.funcs = [func.torch for func in funcs]     # Convert functions to list of PyTorch functions\n",
        "        self.n_double = count_double(funcs)   # Number of activation functions that take 2 inputs\n",
        "        self.n_single = self.n_funcs - self.n_double    # Number of activation functions that take 1 input\n",
        "\n",
        "        self.out_dim = self.n_funcs + self.n_double\n",
        "\n",
        "        if self.initial_weight is not None:     # use the given initial weight\n",
        "            self.W = nn.Parameter(self.initial_weight.clone().detach())  # copies\n",
        "            self.built = True\n",
        "        else:\n",
        "            self.W = torch.normal(mean=0.0, std=init_stddev, size=(in_dim, self.out_dim))\n",
        "\n",
        "    def forward(self, x):  # used to be __call__\n",
        "        \"\"\"Multiply by weight matrix and apply activation units\"\"\"\n",
        "\n",
        "        g = torch.matmul(x, self.W)         # shape = (?, self.size)\n",
        "        self.output = []\n",
        "\n",
        "        in_i = 0    # input index\n",
        "        out_i = 0   # output index\n",
        "        # Apply functions with only a single input\n",
        "        while out_i < self.n_single:\n",
        "            self.output.append(self.funcs[out_i](g[:, in_i]))\n",
        "            in_i += 1\n",
        "            out_i += 1\n",
        "        # Apply functions that take 2 inputs and produce 1 output\n",
        "        while out_i < self.n_funcs:\n",
        "            self.output.append(self.funcs[out_i](g[:, in_i], g[:, in_i+1]))\n",
        "            in_i += 2\n",
        "            out_i += 1\n",
        "\n",
        "        self.output = torch.stack(self.output, dim=1)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def get_weight(self):\n",
        "        return self.W.cpu().detach().numpy()\n",
        "\n",
        "    def get_weight_tensor(self):\n",
        "        return self.W.clone()\n",
        "\n",
        "\n",
        "class SymbolicNet(nn.Module):\n",
        "    \"\"\"Symbolic regression network with multiple layers. Produces one output.\"\"\"\n",
        "    def __init__(self, symbolic_depth, funcs=None, initial_weights=None, init_stddev=0.1):\n",
        "        super(SymbolicNet, self).__init__()\n",
        "\n",
        "        self.depth = symbolic_depth     # Number of hidden layers\n",
        "        self.funcs = funcs\n",
        "        layer_in_dim = [1] + self.depth*[len(funcs)]\n",
        "\n",
        "        if initial_weights is not None:\n",
        "            layers = [SymbolicLayer(funcs=funcs, initial_weight=initial_weights[i], in_dim=layer_in_dim[i])\n",
        "                      for i in range(self.depth)]\n",
        "            self.output_weight = nn.Parameter(initial_weights[-1].clone().detach())\n",
        "\n",
        "        else:\n",
        "            # Each layer initializes its own weights\n",
        "            if not isinstance(init_stddev, list):\n",
        "                init_stddev = [init_stddev] * self.depth\n",
        "            layers = [SymbolicLayer(funcs=self.funcs, init_stddev=init_stddev[i], in_dim=layer_in_dim[i])\n",
        "                      for i in range(self.depth)]\n",
        "\n",
        "            # Initialize weights for last layer (without activation functions)\n",
        "            self.output_weight = nn.Parameter(torch.rand((layers[-1].n_funcs, 1)))\n",
        "        self.hidden_layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, input):\n",
        "        h = self.hidden_layers(input)   # Building hidden layers\n",
        "        return torch.matmul(h, self.output_weight)  # Final output (no activation units) of network\n",
        "\n",
        "    def get_weights(self):\n",
        "        \"\"\"Return list of weight matrices\"\"\"\n",
        "        # First part is iterating over hidden weights. Then append the output weight.\n",
        "        return [self.hidden_layers[i].get_weight() for i in range(self.depth)] + \\\n",
        "               [self.output_weight.cpu().detach().numpy()]\n",
        "\n",
        "    def get_weights_tensor(self):\n",
        "        \"\"\"Return list of weight matrices as tensors\"\"\"\n",
        "        return [self.hidden_layers[i].get_weight_tensor() for i in range(self.depth)] + \\\n",
        "               [self.output_weight.clone()]\n"
      ],
      "metadata": {
        "id": "12EsIG99ppkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*With Regularization*"
      ],
      "metadata": {
        "id": "V4i7gwRZCtag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SymbolicLayerL0(SymbolicLayer):\n",
        "    def __init__(self, in_dim=None, funcs=None, initial_weight=None, init_stddev=0.1,\n",
        "                 bias=False, droprate_init=0.5, lamba=1.,\n",
        "                 beta=2 / 3, gamma=-0.1, zeta=1.1, epsilon=1e-6):\n",
        "        super().__init__(in_dim=in_dim, funcs=funcs, initial_weight=initial_weight, init_stddev=init_stddev)\n",
        "\n",
        "        self.droprate_init = droprate_init if droprate_init != 0 else 0.5\n",
        "        self.use_bias = bias\n",
        "        self.lamba = lamba\n",
        "        self.bias = None\n",
        "        self.in_dim = in_dim\n",
        "        self.eps = None\n",
        "\n",
        "        self.beta = beta\n",
        "        self.gamma = gamma\n",
        "        self.zeta = zeta\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = nn.Parameter(0.1 * torch.ones((1, self.out_dim)))\n",
        "        self.qz_log_alpha = nn.Parameter(torch.normal(mean=np.log(1 - self.droprate_init) - np.log(self.droprate_init),\n",
        "                                                      std=1e-2, size=(in_dim, self.out_dim)))\n",
        "\n",
        "    def quantile_concrete(self, u):\n",
        "        \"\"\"Quantile, aka inverse CDF, of the 'stretched' concrete distribution\"\"\"\n",
        "        y = torch.sigmoid((torch.log(u) - torch.log(1.0 - u) + self.qz_log_alpha) / self.beta)\n",
        "        return y * (self.zeta - self.gamma) + self.gamma\n",
        "\n",
        "    def sample_u(self, shape, reuse_u=False):\n",
        "        \"\"\"Uniform random numbers for concrete distribution\"\"\"\n",
        "        if self.eps is None or not reuse_u:\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            self.eps = torch.rand(size=shape).to(device) * (1 - 2 * self.epsilon) + self.epsilon\n",
        "        return self.eps\n",
        "\n",
        "    def sample_z(self, batch_size, sample=True):\n",
        "        \"\"\"Use the hard concrete distribution as described in https://arxiv.org/abs/1712.01312\"\"\"\n",
        "        if sample:\n",
        "            eps = self.sample_u((batch_size, self.in_dim, self.out_dim))\n",
        "            z = self.quantile_concrete(eps)\n",
        "            return torch.clamp(z, min=0, max=1)\n",
        "        else:  # Mean of the hard concrete distribution\n",
        "            pi = torch.sigmoid(self.qz_log_alpha)\n",
        "            return torch.clamp(pi * (self.zeta - self.gamma) + self.gamma, min=0.0, max=1.0)\n",
        "\n",
        "    def get_z_mean(self):\n",
        "        \"\"\"Mean of the hard concrete distribution\"\"\"\n",
        "        pi = torch.sigmoid(self.qz_log_alpha)\n",
        "        return torch.clamp(pi * (self.zeta - self.gamma) + self.gamma, min=0.0, max=1.0)\n",
        "\n",
        "    def sample_weights(self, reuse_u=False):\n",
        "        z = self.quantile_concrete(self.sample_u((self.in_dim, self.out_dim), reuse_u=reuse_u))\n",
        "        mask = torch.clamp(z, min=0.0, max=1.0)\n",
        "        return mask * self.W\n",
        "\n",
        "    def get_weight(self):\n",
        "        \"\"\"Deterministic value of weight based on mean of z\"\"\"\n",
        "        return self.W * self.get_z_mean()\n",
        "\n",
        "    def loss(self):\n",
        "        \"\"\"Regularization loss term\"\"\"\n",
        "        return torch.sum(torch.sigmoid(self.qz_log_alpha - self.beta * np.log(-self.gamma / self.zeta)))\n",
        "\n",
        "    def forward(self, x, sample=True, reuse_u=False):\n",
        "        \"\"\"Multiply by weight matrix and apply activation units\"\"\"\n",
        "        if sample:\n",
        "            h = torch.matmul(x, self.sample_weights(reuse_u=reuse_u))\n",
        "        else:\n",
        "            w = self.get_weight()\n",
        "            h = torch.matmul(x, w)\n",
        "\n",
        "        if self.use_bias:\n",
        "            h = h + self.bias\n",
        "\n",
        "        # shape of h = (?, self.n_funcs)\n",
        "\n",
        "        output = []\n",
        "        # apply a different activation unit to each column of h\n",
        "        in_i = 0  # input index\n",
        "        out_i = 0  # output index\n",
        "        # Apply functions with only a single input\n",
        "        while out_i < self.n_single:\n",
        "            output.append(self.funcs[out_i](h[:, in_i]))\n",
        "            in_i += 1\n",
        "            out_i += 1\n",
        "        # Apply functions that take 2 inputs and produce 1 output\n",
        "        while out_i < self.n_funcs:\n",
        "            output.append(self.funcs[out_i](h[:, in_i], h[:, in_i + 1]))\n",
        "            in_i += 2\n",
        "            out_i += 1\n",
        "        output = torch.stack(output, dim=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "class SymbolicNetL0(nn.Module):\n",
        "    \"\"\"Symbolic regression network with multiple layers. Produces one output.\"\"\"\n",
        "\n",
        "    def __init__(self, symbolic_depth, in_dim=1, funcs=None, initial_weights=None, init_stddev=0.1):\n",
        "        super(SymbolicNetL0, self).__init__()\n",
        "        self.depth = symbolic_depth  # Number of hidden layers\n",
        "        self.funcs = funcs\n",
        "\n",
        "        layer_in_dim = [in_dim] + self.depth * [len(funcs)]\n",
        "        if initial_weights is not None:\n",
        "            layers = [SymbolicLayerL0(funcs=funcs, initial_weight=initial_weights[i],\n",
        "                                      in_dim=layer_in_dim[i])\n",
        "                      for i in range(self.depth)]\n",
        "            self.output_weight = nn.Parameter(initial_weights[-1].clone().detach())\n",
        "        else:\n",
        "            # Each layer initializes its own weights\n",
        "            if not isinstance(init_stddev, list):\n",
        "                init_stddev = [init_stddev] * self.depth\n",
        "            layers = [SymbolicLayerL0(funcs=funcs, init_stddev=init_stddev[i], in_dim=layer_in_dim[i])\n",
        "                      for i in range(self.depth)]\n",
        "            # Initialize weights for last layer (without activation functions)\n",
        "            self.output_weight = nn.Parameter(torch.rand(size=(self.hidden_layers[-1].n_funcs, 1)) * 2)\n",
        "        self.hidden_layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, input, sample=True, reuse_u=False):\n",
        "        # connect output from previous layer to input of next layer\n",
        "        h = input\n",
        "        for i in range(self.depth):\n",
        "            h = self.hidden_layers[i](h, sample=sample, reuse_u=reuse_u)\n",
        "\n",
        "        h = torch.matmul(h, self.output_weight)     # Final output (no activation units) of network\n",
        "        return h\n",
        "\n",
        "    def get_loss(self):\n",
        "        return torch.sum(torch.stack([self.hidden_layers[i].loss() for i in range(self.depth)]))\n",
        "\n",
        "    def get_weights(self):\n",
        "        \"\"\"Return list of weight matrices\"\"\"\n",
        "        # First part is iterating over hidden weights. Then append the output weight.\n",
        "        return [self.hidden_layers[i].get_weight().cpu().detach().numpy() for i in range(self.depth)] + \\\n",
        "               [self.output_weight.cpu().detach().numpy()]"
      ],
      "metadata": {
        "id": "YktS22bhpsBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **I.3 - Symbolic Expression of the EQL Network**"
      ],
      "metadata": {
        "id": "tZj63b2XhW0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Generate a mathematical expression of the symbolic regression network (AKA EQL network) using SymPy. This expression\n",
        "can be used to pretty-print the expression (including human-readable text, LaTeX, etc.). SymPy also allows algebraic\n",
        "manipulation of the expression.\n",
        "The main function is network(...)\n",
        "There are several filtering functions to simplify expressions, although these are not always needed if the weight matrix\n",
        "is already pruned.\n",
        "\"\"\"\n",
        "\n",
        "import sympy as sym\n",
        "\n",
        "def apply_activation(W, funcs, n_double=0):\n",
        "    \"\"\"Given an (n, m) matrix W and (m) vector of funcs, apply funcs to W.\n",
        "\n",
        "    Arguments:\n",
        "        W:  (n, m) matrix\n",
        "        funcs: list of activation functions (SymPy functions)\n",
        "        n_double:   Number of activation functions that take in 2 inputs\n",
        "\n",
        "    Returns:\n",
        "        SymPy matrix with 1 column that represents the output of applying the activation functions.\n",
        "    \"\"\"\n",
        "    W = sym.Matrix(W)\n",
        "    if n_double == 0:\n",
        "        for i in range(W.shape[0]):\n",
        "            for j in range(W.shape[1]):\n",
        "                W[i, j] = funcs[j](W[i, j])\n",
        "    else:\n",
        "        W_new = W.copy()\n",
        "        out_size = len(funcs)\n",
        "        for i in range(W.shape[0]):\n",
        "            in_j = 0\n",
        "            out_j = 0\n",
        "            while out_j < out_size - n_double:\n",
        "                W_new[i, out_j] = funcs[out_j](W[i, in_j])\n",
        "                in_j += 1\n",
        "                out_j += 1\n",
        "            while out_j < out_size:\n",
        "                W_new[i, out_j] = funcs[out_j](W[i, in_j], W[i, in_j+1])\n",
        "                in_j += 2\n",
        "                out_j += 1\n",
        "        for i in range(n_double):\n",
        "            W_new.col_del(-1)\n",
        "        W = W_new\n",
        "    return W\n",
        "\n",
        "\n",
        "def sym_pp(W_list, funcs, var_names, threshold=0.01, n_double=0):\n",
        "    \"\"\"Pretty print the hidden layers (not the last layer) of the symbolic regression network\n",
        "\n",
        "    Arguments:\n",
        "        W_list: list of weight matrices for the hidden layers\n",
        "        funcs:  list of lambda functions using sympy. has the same size as W_list[i][j, :]\n",
        "        var_names: list of strings for names of variables\n",
        "        threshold: threshold for filtering expression. set to 0 for no filtering.\n",
        "        n_double:   Number of activation functions that take in 2 inputs\n",
        "\n",
        "    Returns:\n",
        "        Simplified sympy expression.\n",
        "    \"\"\"\n",
        "    vars = []\n",
        "    for var in var_names:\n",
        "        if isinstance(var, str):\n",
        "            vars.append(sym.Symbol(var))\n",
        "        else:\n",
        "            vars.append(var)\n",
        "    expr = sym.Matrix(vars).T\n",
        "    # W_list = np.asarray(W_list)\n",
        "    for W in W_list:\n",
        "        W = filter_mat(sym.Matrix(W), threshold=threshold)\n",
        "        expr = expr * W\n",
        "        expr = apply_activation(expr, funcs, n_double=n_double)\n",
        "    # expr = expr * W_list[-1]\n",
        "    return expr\n",
        "\n",
        "\n",
        "def last_pp(eq, W):\n",
        "    \"\"\"Pretty print the last layer.\"\"\"\n",
        "    return eq * filter_mat(sym.Matrix(W))\n",
        "\n",
        "\n",
        "def network(weights, funcs, var_names, threshold=0.01):\n",
        "    \"\"\"Pretty print the entire symbolic regression network.\n",
        "\n",
        "    Arguments:\n",
        "        weights: list of weight matrices for the entire network\n",
        "        funcs:  list of lambda functions using sympy. has the same size as W_list[i][j, :]\n",
        "        var_names: list of strings for names of variables\n",
        "        threshold: threshold for filtering expression. set to 0 for no filtering.\n",
        "\n",
        "    Returns:\n",
        "        Simplified sympy expression.\"\"\"\n",
        "    n_double = count_double(funcs)\n",
        "    funcs = [func.sp for func in funcs]\n",
        "\n",
        "    expr = sym_pp(weights[:-1], funcs, var_names, threshold=threshold, n_double=n_double)\n",
        "    expr = last_pp(expr, weights[-1])\n",
        "    expr = expr[0, 0]\n",
        "    return expr\n",
        "\n",
        "\n",
        "def filter_mat(expr, threshold=0.01):\n",
        "    \"\"\"Sets all constants under threshold to 0\n",
        "    TODO: Test\"\"\"\n",
        "    for a in sym.preorder_traversal(expr):\n",
        "        if isinstance(a, sym.Float) and a < threshold:\n",
        "            expr = expr.subs(a, 0)\n",
        "    return expr"
      ],
      "metadata": {
        "id": "qLOMHwIEtQTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **I.4 - Test of the EQL Network**"
      ],
      "metadata": {
        "id": "piQVmTBLhnhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Trains the deep symbolic regression architecture on given functions to produce a simple equation that describes\n",
        "the dataset. Uses L_0 regularization for the EQL network.\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from inspect import signature\n",
        "import time\n",
        "\n",
        "\n",
        "N_TRAIN = 100       # Size of training dataset\n",
        "N_VAL = 100         # Size of validation dataset\n",
        "DOMAIN = (-3, 3)    # Domain of dataset - range from which we sample x\n",
        "# DOMAIN = np.array([[0, -1, -1], [1, 1, 1]])   # Use this format if each input variable has a different domain\n",
        "N_TEST = 100        # Size of test dataset\n",
        "DOMAIN_TEST = (-3, 3)   # Domain of test dataset - should be larger than training domain to test extrapolation\n",
        "NOISE_SD = 0        # Standard deviation of noise for training dataset\n",
        "var_names = [\"x\", \"y\", \"z\"]\n",
        "\n",
        "# Standard deviation of random distribution for weight initializations.\n",
        "init_sd_first = 0.1\n",
        "init_sd_last = 1.0\n",
        "init_sd_middle = 0.5\n",
        "# init_sd_first = 0.5\n",
        "# init_sd_last = 0.5\n",
        "# init_sd_middle = 0.5\n",
        "# init_sd_first = 0.1\n",
        "# init_sd_last = 0.1\n",
        "# init_sd_middle = 0.1\n",
        "\n",
        "\n",
        "def generate_data(func, N, range_min=DOMAIN[0], range_max=DOMAIN[1]):\n",
        "    \"\"\"Generates datasets.\"\"\"\n",
        "    x_dim = len(signature(func).parameters)     # Number of inputs to the function, or, dimensionality of x\n",
        "    x = (range_max - range_min) * torch.rand([N, x_dim]) + range_min\n",
        "    y = torch.tensor([[func(*x_i)] for x_i in x])\n",
        "    return x, y\n",
        "\n",
        "\n",
        "class Benchmark:\n",
        "\n",
        "    def __init__(self, n_layers=1, reg_weight=5e-3, learning_rate=1e-2,\n",
        "                 n_epochs1=10001, n_epochs2=10001):\n",
        "        \"\"\"Set hyper-parameters\"\"\"\n",
        "        self.activation_funcs = [\n",
        "            *[Identity()] * 1,\n",
        "            *[Sin()] * 1,\n",
        "        ]\n",
        "\n",
        "        self.n_layers = n_layers                # Number of hidden layers\n",
        "        self.reg_weight = reg_weight            # Regularization weight\n",
        "        self.learning_rate = learning_rate\n",
        "        self.summary_step = 1000                # Number of iterations at which to print to screen\n",
        "        self.n_epochs1 = n_epochs1\n",
        "        self.n_epochs2 = n_epochs2\n",
        "\n",
        "    def benchmark(self, func, func_name, trials):\n",
        "        \"\"\"Benchmark the EQL network on data generated by the given function. Print the results ordered by test error.\n",
        "\n",
        "        Arguments:\n",
        "            func: lambda function to generate dataset\n",
        "            func_name: string that describes the function - this will be the directory name\n",
        "            trials: number of trials to train from scratch. Will save the results for each trial.\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Starting benchmark for function:\\t%s\" % func_name)\n",
        "        print(\"==============================================\")\n",
        "\n",
        "        # Train network!\n",
        "        expr_list, error_test_list = self.train(func, func_name, trials)\n",
        "\n",
        "        # Sort the results by test error (increasing) and print them to file\n",
        "        # This allows us to easily count how many times it fit correctly.\n",
        "        error_expr_sorted = sorted(zip(error_test_list, expr_list))     # List of (error, expr)\n",
        "        error_test_sorted = [x for x, _ in error_expr_sorted]   # Separating out the errors\n",
        "        expr_list_sorted = [x for _, x in error_expr_sorted]    # Separating out the expr\n",
        "\n",
        "    def train(self, func, func_name='', trials=1):\n",
        "        \"\"\"Train the network to find a given function\"\"\"\n",
        "\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "        print(\"Use cuda:\", use_cuda, \"Device:\", device)\n",
        "\n",
        "        x, y = generate_data(func, N_TRAIN)\n",
        "        data, target = x.to(device), y.to(device)\n",
        "        # x_val, y_val = generate_data(func, N_VAL)\n",
        "        x_test, y_test = generate_data(func, N_TEST, range_min=DOMAIN_TEST[0], range_max=DOMAIN_TEST[1])\n",
        "        test_data, test_target = x_test.to(device), y_test.to(device)\n",
        "\n",
        "        # Setting up the symbolic regression network\n",
        "        x_dim = len(signature(func).parameters)  # Number of input arguments to the function\n",
        "\n",
        "        width = len(self.activation_funcs)\n",
        "        n_double = count_double(self.activation_funcs)\n",
        "\n",
        "        # Arrays to keep track of various quantities as a function of epoch\n",
        "        loss_list = []          # Total loss (MSE + regularization)\n",
        "        error_list = []         # MSE\n",
        "        reg_list = []           # Regularization\n",
        "        error_test_list = []    # Test error\n",
        "\n",
        "        error_test_final = []\n",
        "        eq_list = []\n",
        "\n",
        "        for trial in range(trials):\n",
        "            print(\"Training on function \" + func_name + \" Trial \" + str(trial+1) + \" out of \" + str(trials))\n",
        "\n",
        "            # reinitialize for each trial\n",
        "            net = SymbolicNetL0(self.n_layers,\n",
        "                                funcs=self.activation_funcs,\n",
        "                                initial_weights=[\n",
        "                                    # kind of a hack for truncated normal\n",
        "                                    torch.fmod(torch.normal(0, init_sd_first, size=(x_dim, width + n_double)), 3),\n",
        "                                    torch.fmod(torch.normal(0, init_sd_middle, size=(width, width + n_double)), 3),\n",
        "                                    torch.fmod(torch.normal(0, init_sd_middle, size=(width, width + n_double)), 3),\n",
        "                                    torch.fmod(torch.normal(0, init_sd_last, size=(width, 1)), 3)\n",
        "                                ]).to(device)\n",
        "\n",
        "            loss_val = np.nan\n",
        "            while np.isnan(loss_val):\n",
        "                # training restarts if gradients blow up\n",
        "                criterion = nn.MSELoss()\n",
        "                optimizer = optim.RMSprop(net.parameters(),\n",
        "                                          lr=self.learning_rate * 10,\n",
        "                                          alpha=0.9,  # smoothing constant\n",
        "                                          eps=1e-10,\n",
        "                                          momentum=0.0,\n",
        "                                          centered=False)\n",
        "\n",
        "                # adapative learning rate\n",
        "                lmbda = lambda epoch: 0.1\n",
        "                scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
        "                # for param_group in optimizer.param_groups:\n",
        "                #     print(\"Learning rate: %f\" % param_group['lr'])\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                # 0th warmup stage, then 2 stages of training with decreasing learning rate\n",
        "                for epoch in range(self.n_epochs1 + self.n_epochs2 + 2000):\n",
        "                    optimizer.zero_grad()  # zero the parameter gradients\n",
        "                    outputs = net(data)  # forward pass\n",
        "                    mse_loss = criterion(outputs, target)\n",
        "\n",
        "                    reg_loss = net.get_loss()\n",
        "                    loss = mse_loss + self.reg_weight * reg_loss\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    if epoch % self.summary_step == 0:\n",
        "                        error_val = mse_loss.item()\n",
        "                        reg_val = reg_loss.item()\n",
        "                        loss_val = loss.item()\n",
        "                        error_list.append(error_val)\n",
        "                        reg_list.append(reg_val)\n",
        "                        loss_list.append(loss_val)\n",
        "\n",
        "                        with torch.no_grad():  # test error\n",
        "                            test_outputs = net(test_data)\n",
        "                            test_loss = F.mse_loss(test_outputs, test_target)\n",
        "                            error_test_val = test_loss.item()\n",
        "                            error_test_list.append(error_test_val)\n",
        "\n",
        "                        print(\"Epoch: %d\\tTotal training loss: %f\\tTest error: %f\" % (epoch, loss_val, error_test_val))\n",
        "\n",
        "                        if np.isnan(loss_val):  # If loss goes to NaN, restart training\n",
        "                            break\n",
        "\n",
        "                    if epoch == 2000:\n",
        "                        scheduler.step()  # lr /= 10\n",
        "                    elif epoch == self.n_epochs1 + 2000:\n",
        "                        scheduler.step()    # lr /= 10 again\n",
        "\n",
        "                scheduler.step()  # lr /= 10 again\n",
        "\n",
        "                t1 = time.time()\n",
        "\n",
        "            tot_time = t1 - t0\n",
        "            print(tot_time)\n",
        "\n",
        "            # Print the expressions\n",
        "            with torch.no_grad():\n",
        "                weights = net.get_weights()\n",
        "                expr = network(weights, self.activation_funcs, var_names[:x_dim])\n",
        "                print(expr)\n",
        "\n",
        "            error_test_final.append(error_test_list[-1])\n",
        "            eq_list.append(expr)\n",
        "\n",
        "        return eq_list, error_test_final\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    kwargs = {\n",
        "        \"n_layers\": 2,\n",
        "        \"reg_weight\": 5e-6,\n",
        "        \"learning_rate\": 1e-2,\n",
        "        \"n_epochs1\": 2001,\n",
        "        \"n_epochs2\": 2001\n",
        "    }\n",
        "    print(kwargs)\n",
        "\n",
        "    bench = Benchmark(**kwargs)\n",
        "\n",
        "    bench.benchmark(lambda x: np.sin(x), func_name=\"sin(x)\", trials=5)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDVWx2ClNADX",
        "outputId": "9d552fb4-0805-4d74-fc71-df717539fe21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_layers': 2, 'reg_weight': 5e-06, 'learning_rate': 0.01, 'n_epochs1': 2001, 'n_epochs2': 2001}\n",
            "Starting benchmark for function:\tsin(x)\n",
            "==============================================\n",
            "Use cuda: False Device: cpu\n",
            "Training on function sin(x) Trial 1 out of 5\n",
            "Epoch: 0\tTotal training loss: 0.271112\tTest error: 0.553456\n",
            "Epoch: 1000\tTotal training loss: 0.103743\tTest error: 0.023668\n",
            "Epoch: 2000\tTotal training loss: 0.035727\tTest error: 0.091404\n",
            "Epoch: 3000\tTotal training loss: 0.002403\tTest error: 0.002452\n",
            "Epoch: 4000\tTotal training loss: 0.001970\tTest error: 0.001726\n",
            "Epoch: 5000\tTotal training loss: 0.000141\tTest error: 0.000119\n",
            "Epoch: 6000\tTotal training loss: 0.000045\tTest error: 0.000337\n",
            "17.108752489089966\n",
            "0.0504331*x + 0.920519*sin(1.04194128263668*x)\n",
            "Training on function sin(x) Trial 2 out of 5\n",
            "Epoch: 0\tTotal training loss: 0.471166\tTest error: 0.571676\n",
            "Epoch: 1000\tTotal training loss: 0.029539\tTest error: 0.037158\n",
            "Epoch: 2000\tTotal training loss: 0.041985\tTest error: 0.061174\n",
            "Epoch: 3000\tTotal training loss: 0.001953\tTest error: 0.002150\n",
            "Epoch: 4000\tTotal training loss: 0.000889\tTest error: 0.000953\n",
            "Epoch: 5000\tTotal training loss: 0.005808\tTest error: 0.000174\n",
            "Epoch: 6000\tTotal training loss: 0.000047\tTest error: 0.000053\n",
            "16.409795999526978\n",
            "0\n",
            "Training on function sin(x) Trial 3 out of 5\n",
            "Epoch: 0\tTotal training loss: 0.349161\tTest error: 3.062062\n",
            "Epoch: 1000\tTotal training loss: 0.170358\tTest error: 0.179671\n",
            "Epoch: 2000\tTotal training loss: 0.171390\tTest error: 0.184685\n",
            "Epoch: 3000\tTotal training loss: 0.158579\tTest error: 0.149106\n",
            "Epoch: 4000\tTotal training loss: 0.158166\tTest error: 0.158882\n",
            "Epoch: 5000\tTotal training loss: 0.158305\tTest error: 0.149189\n",
            "Epoch: 6000\tTotal training loss: 0.158213\tTest error: 0.149886\n",
            "16.89432191848755\n",
            "0\n",
            "Training on function sin(x) Trial 4 out of 5\n",
            "Epoch: 0\tTotal training loss: 0.910706\tTest error: 0.502660\n",
            "Epoch: 1000\tTotal training loss: 0.164367\tTest error: 0.151708\n",
            "Epoch: 2000\tTotal training loss: 0.158183\tTest error: 0.150670\n",
            "Epoch: 3000\tTotal training loss: 0.159472\tTest error: 0.150859\n",
            "Epoch: 4000\tTotal training loss: 0.146256\tTest error: 0.150155\n",
            "Epoch: 5000\tTotal training loss: 0.157084\tTest error: 0.148457\n",
            "Epoch: 6000\tTotal training loss: 0.157747\tTest error: 0.150056\n",
            "17.0004563331604\n",
            "0.327944*x\n",
            "Training on function sin(x) Trial 5 out of 5\n",
            "Epoch: 0\tTotal training loss: 0.427830\tTest error: 0.547701\n",
            "Epoch: 1000\tTotal training loss: 0.162517\tTest error: 0.146017\n",
            "Epoch: 2000\tTotal training loss: 0.159874\tTest error: 0.168196\n",
            "Epoch: 3000\tTotal training loss: 0.156403\tTest error: 0.145491\n",
            "Epoch: 4000\tTotal training loss: 0.161355\tTest error: 0.148579\n",
            "Epoch: 5000\tTotal training loss: 0.156207\tTest error: 0.144378\n",
            "Epoch: 6000\tTotal training loss: 0.156227\tTest error: 0.144522\n",
            "17.568002700805664\n",
            "0.334827*x\n"
          ]
        }
      ]
    }
  ]
}